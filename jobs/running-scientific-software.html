

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Running scientific software &mdash; Sigma2/Metacenter documentation  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Monitoring jobs" href="monitoring.html" />
    <link rel="prev" title="Job Scripts" href="job_scripts.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Sigma2/Metacenter documentation
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">News</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://opslog.sigma2.no">Latest changes and events</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/hardware-status">Hardware live status</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/support_line.html">Support line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/lost_forgotten_password.html">Lost or expiring password</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/project_leader_support.html">Project leader support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/advanced_user_support.html">Advanced User Support (AUS)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/publish_research_data.html">Publish Research Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../frontpage.html">The Norwegian Academic HPC Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_account.html">How do I get an HPC account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/training.html">Training material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/create_ssh_keys.html">SSH</a></li>
</ul>
<p class="caption"><span class="caption-text">HPC Machines</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/fram.html">Fram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference external" href="https://hpc-uit.readthedocs.io">Stallo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/migration2metacenter.html">Migration to a Metacenter HPC machine</a></li>
</ul>
<p class="caption"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/modulescheme.html">Software Module Scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/installed_software.html">Installed Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/containers.html">Running Singularity and Docker containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/userinstallsw.html">How can I, as a user, install software for myself or my project with EasyBuild?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/userinstallsw.html#how-can-i-as-user-install-python-packages">How can I as user install Python packages?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/licenses.html">Licenses and access policies</a></li>
</ul>
<p class="caption"><span class="caption-text">Jobs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="submitting.html">Submitting jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="choosing_job_types.html">Job Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="job_scripts.html">Job Scripts</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Running scientific software</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hardware">Hardware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#interconnect-infiniband">Interconnect - InfiniBand</a></li>
<li class="toctree-l4"><a class="reference internal" href="#processors-and-cores">Processors and cores</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-numa-and-ccnuma">Memory - NUMA and ccNUMA</a></li>
<li class="toctree-l4"><a class="reference internal" href="#avx2-vector-units">AVX2 vector units</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#slurm">Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#storage-for-scratch-during-a-run-on-saga">Storage for scratch during a run on Saga</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pure-mpi">Pure MPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hybrid-mpi-openmp">Hybrid MPI + OpenMP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#srun-vs-mpirun">srun vs mpirun</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#library-settings">Library settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory-architecture">Memory architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-mpi-applications">Running MPI applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#binding">Binding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#intel-mpi">Intel MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openmpi">OpenMPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#collective-mpi-operations-optimisation">Collective MPI operations optimisation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-mpi-only-applications">Running MPI only applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Intel MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">OpenMPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-bandwidth-sensitive-applications">Memory bandwidth sensitive applications</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-mpi-openmp-hybrid-applications">Running  MPI-OpenMP - Hybrid applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">Intel MPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">OpenMPI</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#create-a-hostfile-or-machinefile">Create a hostfile or machinefile</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transport-options-openmpi">Transport options OpenMPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#monitoring-process-thread-placement">Monitoring process/thread placement</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="choosing_memory_settings.html">How to choose the right amount of memory</a></li>
<li class="toctree-l1"><a class="reference internal" href="common_job_failures.html">Common job failures</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">How to check the performance and scaling using Arm Performance Reports</a></li>
<li class="toctree-l1"><a class="reference internal" href="interactive_jobs.html">Interactive jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="projects_accounting.html">Projects and accounting</a></li>
<li class="toctree-l1"><a class="reference internal" href="network-access.html">No network access on compute nodes</a></li>
<li class="toctree-l1"><a class="reference internal" href="dos_and_donts.html">Dos and Don’ts</a></li>
<li class="toctree-l1"><a class="reference internal" href="guides.html">Guides</a></li>
</ul>
<p class="caption"><span class="caption-text">Files and Storage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird.html">NIRD - National Infrastructure for Research Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/backup.html">Backup on Betzy, Fram, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/performance.html">Storage performance tuning</a></li>
</ul>
<p class="caption"><span class="caption-text">Code Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code_development/building.html">Building scientific software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/compilers.html">Compilers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/performance.html">Performance Analysis and Tuning</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Sigma2/Metacenter documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Running scientific software</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/UNINETTSigma2/documentation/blob/master/jobs/running-scientific-software.md" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="running-scientific-software">
<h1>Running scientific software<a class="headerlink" href="#running-scientific-software" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Running scientific software on current modern HPC systems is a demanding task
even for experts. The system design is quite complex with interconnects, nodes,
shared and distributed memory on a range of levels, a large number of both
cores with threading within a core and not to mention all the software
libraries and run time settings.</p>
<p>This document provides some background information to better understand details
of various system compenents, and tries to illustrate how to make use of that
information for using modern HPC systems in the best possible way.</p>
<div class="section" id="hardware">
<h3>Hardware<a class="headerlink" href="#hardware" title="Permalink to this headline">¶</a></h3>
<div class="section" id="interconnect-infiniband">
<h4>Interconnect - InfiniBand<a class="headerlink" href="#interconnect-infiniband" title="Permalink to this headline">¶</a></h4>
<p>A cluster (using Betzy as the example) contains a rather large number of nodes
(Betzy 1344) with an interconnect that enables efficient delivery of messages
(Message Passing Interface, MPI) between the nodes. On Betzy Mellanox
InfiniBand is used, in a HDR-100 configuration.  The HDR (High Data Rate)
standard is 200 Gbits/s and HDR-100 is half of this. This is a tradeoff, as
each switch port can accommodate two compute nodes. All the compute nodes are
connected in a Dragonfly topology. (While not fully symmetrical, tests have
shown that the slowdown by spreading the ranks randomly around the compute
nodes had less than the tender specified 10%. Acceptance tests showed from 8 to
zero percent slow down depending on the application. Hence for all practical
purposes there is no need to pay special attention to schedule jobs within a
single rack/cell etc).</p>
</div>
<div class="section" id="processors-and-cores">
<h4>Processors and cores<a class="headerlink" href="#processors-and-cores" title="Permalink to this headline">¶</a></h4>
<p>Each compute node contains two sockets with a 64 core AMD processor per socket.
Every processor has 64 cores each supporting 2-way <em>simultaneous multithreading</em>
(SMT, see https://en.wikipedia.org/wiki/Simultaneous_multithreading for more
information). To not confuse these <em>threading</em> capabilities in hardware with
threads in software (e.g., pthreads or OpenMP), we use the term <em>virtual core</em>
from now on.</p>
<p>For applications it looks as if every compute node has 256 independent
<em>virtual cores</em> numbered from 0 to 255. Due to SMT, always two of these seemingly
independent virtual cores form a pair and share the executing units of a core. If both of
these two virtual cores are used in parallel by an application, the
application’s performance will be the same as if it used only one virtual core
(and the other one is left idle) or if two different applications would use one
virtual core each, each of the two applications would achieve only half of the
performance of a core. To achieve the maximum performance from each core, it is
therefore important to pay attention to the mapping of processes to cores, that is,
<strong>any two processes (or software threads) of an application must not share the
two virtual cores</strong> or, in other words, one of the two virtual cores in a pair
shall be kept idle.</p>
<p>The following command provides information about the numbering of virtual cores:
<code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">/proc/cpuinfo</span> <span class="pre">|</span> <span class="pre">sed</span> <span class="pre">'/processor\|physical</span> <span class="pre">id\|core</span> <span class="pre">id/!d'</span> <span class="pre">|</span> <span class="pre">sed</span> <span class="pre">'N;N;s/\n/</span> <span class="pre">/g'</span></code>
The first 128 entries (processor 0-127) correspond to the first virtual core.
Accordingly, the second 128 entries (processor 128-255) correspond to the second
virtual core. So, if one limits the placement of processes to processor
numbers 0-127, no process will share executional units with any other process.</p>
<p>Both Intel MPI and OpenMPI provide means to achieve such placements. See
examples below.</p>
</div>
<div class="section" id="memory-numa-and-ccnuma">
<h4>Memory - NUMA and ccNUMA<a class="headerlink" href="#memory-numa-and-ccnuma" title="Permalink to this headline">¶</a></h4>
<p>Each compute node has 256 GiB of memory being organised in 8 banks of 32 GiB
each. Every processor has four memory controllers each being responsible for
one bank. Furthermore, every virtual core in a processor is assigned to one memory
controller which results in different paths to access memory. Memory accesses
may have to traverse an intra-processor network (another controller within the
same processor is responsible for the memory address being accessed) or an
intra-node network (another controller of the other processor is responsible for
the memory address being accessed). This memory organisation is referred to as
Non Uniform Memory Access (NUMA) memory. A NUMA node comprises of a memory bank
and a subset of the virtual cores. The best performance is achieved when
processes and the memory they access (frequently) are placed close to each other, or
in other words, within one NUMA node.</p>
<p>Additionally, the compute nodes implement cache coherent NUMA (ccNUMA) memory
which ensures that programs see a consistent memory image. Cache coherence requires
intra-node communication if different caches store the same memory location.
Hence, the best performance is achieved when the same memory location is not
stored in different caches. Typically this happens when processes need access to
some shared data, e.g., at boundaries of regions they iterate over. Limiting or even
avoiding these accesses is often a challenge.</p>
<p>To display information about the NUMA nodes use the command <code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">-H</span></code>. More details on this
later.</p>
</div>
<div class="section" id="avx2-vector-units">
<h4>AVX2 vector units<a class="headerlink" href="#avx2-vector-units" title="Permalink to this headline">¶</a></h4>
<p>Each of the processor cores have two vector units, these are 256 bits wide and
can hence operate on four 64-bit floating point numbers simultaneously. With
two such units and selecting fused multiply add (FMA) up to 16 double precision
operations can be performed per clock cycle. (no program contain only FMA
instruction so these numbers are inflated). This yields a marketing theoretical
performance of frequency times number of cores times 16, 2.26 GHz * 128 * 16 =
4608 Gflops/s for a single compute node (or 6.2 Pflops/s for the complete
Betzy). In any case the vector units are important for floating point
performance, see the note on environment flag for MKL later.</p>
</div>
</div>
</div>
<div class="section" id="slurm">
<h2>Slurm<a class="headerlink" href="#slurm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Introduction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The resource manager used by the Sigma2 systems is Slurm.</p>
<p>Requesting the correct set of resources in the Slurm job script is vital for
optimum performance. The job may be pure MPI or a hybrid using both MPI and
OpenMPI.</p>
<p>In addition we have seen that in some cases memory bandwidth is the performance
limiting factor and only 64 processes per compute node is requested.</p>
<p>All of this is set in the Slurm job script. The job script can be written in
any language that uses <code class="docutils literal notranslate"><span class="pre">#</span></code> as the comment sign. Bash is most common, but some
applications like NorESM use Python. Perl, Julia and R are other options. Here
is a simple Python example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>

<span class="c1">#SBATCH --job-name=slurm</span>
<span class="c1">#SBATCH --account=nn9999k</span>
<span class="c1">#SBATCH --nodes=128</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>
<span class="c1">#SBATCH --time=0:0:5</span>

<span class="kn">import</span> <span class="nn">os</span>

<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;srun hostname&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Using Python, Perl, Julia or R opens up for far more programming within the run
script than what’s possible using bash.</p>
<p>Some examples for Betzy are given below, for details about Slurm see the
<a class="reference internal" href="job_scripts/slurm_parameter.html"><span class="doc std std-doc">documentation about Slurm</span></a>.</p>
</div>
<div class="section" id="storage-for-scratch-during-a-run-on-saga">
<h3>Storage for scratch during a run on Saga<a class="headerlink" href="#storage-for-scratch-during-a-run-on-saga" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Scratch storage is only available on Saga.
See also <a class="reference internal" href="../files_storage/clusters.html"><span class="doc std std-doc">the overview of storage areas</span></a>.</p>
</div>
<p>Scratch storage for read and write files or any short lived files or files that are read and written to during a run should reside on a scratch pad area, <a class="reference internal" href="../files_storage/clusters.html"><span class="doc std std-doc">storage on clusters</span></a>.</p>
<p>On Saga there are two scratch areas for such files (Fram and Betzy only has shared scratch), one shared for all processes and nodes which is residing on the parallel file system and another which is local on each compute node. The latter is smaller and only accessible to processes running on that node.</p>
<p>There are benefits for both types depending on usage. The parallel file system is by nature slower for random read &amp; write operations and metadata operations (handling of large number fo files), the local file system is far better suited for this. In addition the shared file system need to serve all users and placing very high metadata load on it make the file system slow for all users. However, not only does aggregate performace scale with the number of nodes used but it does not affect the other users when using local scratch.</p>
<p>Which kind of scratch file system to use is a tradeoff, if you need sharing or a large amount of data (more than 250-300 GB) there is only one option, shared scratch. If on the other hand you have a lot of random IO or a large number of files then local scratch is much better suited.</p>
<p>All SLURM jobs get allocated a shared scratch file system pointed to by the variable $SCRATCH , but you need to ask for local scratch, like this example where I have asked for 100 Gigabytes of local scratch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --gres=localscratch:100G</span>
</pre></div>
</div>
<p>By including this in the job script each allocated node will have a local scratch area pointed to by the variable $LOCALSCRATCH, (on Saga the maximum you can ask for is about 300 GB). Please do not ask for more than what you actually need, other users might share the local scratch space with you.</p>
<p>Saga has spinning disks with limited performance (but more nodes give more performance) for local scratch, but newer systems will have memory chips based storage for localscratch and hence it’s good practice to start using local scratch now.</p>
</div>
<div class="section" id="pure-mpi">
<h3>Pure MPI<a class="headerlink" href="#pure-mpi" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --ntasks=4096</span>
<span class="c1">#SBATCH --nodes=32</span>
<span class="c1">#SBATCH --ntasks-per-node=128</span>
</pre></div>
</div>
<p>This will request 32 nodes with 128 ranks per compute nodes giving a total of 4
ki ranks. The <code class="docutils literal notranslate"><span class="pre">--ntasks</span></code> is not strictly needed (if missing, it will be
calculated from <code class="docutils literal notranslate"><span class="pre">--nodes</span></code> and <code class="docutils literal notranslate"><span class="pre">--ntasks-per-node</span></code>.)</p>
<p>To get the total number of cores in a pure MPI job script the environment
variable <code class="docutils literal notranslate"><span class="pre">$SLURM_NTASKS</span></code> is available.</p>
</div>
<div class="section" id="hybrid-mpi-openmp">
<h3>Hybrid MPI + OpenMP<a class="headerlink" href="#hybrid-mpi-openmp" title="Permalink to this headline">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodes=1200</span>
<span class="c1">#SBATCH --ntasks-per-node=8</span>
<span class="c1">#SBATCH --cpus-per-task=16</span>
</pre></div>
</div>
<p>This will request 1200 nodes placing 8 MPI ranks per node and provide 16 OpenMP
threads to each MPI rank, a total of 128 cores per compute node. 1200 times 128
is 153600 or 150 ki cores.</p>
<p>To get the total number of cores in a Hybrid MPI + OpenMP job script one can
multiply the environment variables <code class="docutils literal notranslate"><span class="pre">$SLURM_NTASKS</span></code> and <code class="docutils literal notranslate"><span class="pre">$SLURM_CPUS_PER_TASK</span></code>.</p>
<p>To generate a list of all the Slurm variables just issue an <code class="docutils literal notranslate"><span class="pre">env</span></code> command in
the job script  and all environment variables will be listed.</p>
<div class="section" id="srun-vs-mpirun">
<h4>srun vs mpirun<a class="headerlink" href="#srun-vs-mpirun" title="Permalink to this headline">¶</a></h4>
<p>Most if the times the <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command can be used. The <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> sets up the
MPI environment and makes sure that everything is ready for the MPI function
<code class="docutils literal notranslate"><span class="pre">MPI_init()</span></code> when it’s called in the start of any MPI program.</p>
<p>As Slurm is built with MPI support srun will also set up the MPI environment.</p>
<p>Both mpirun and srun launch the executable on the requested nodes. While there
is a large range of opinions on this matter it’s hard to make a final statement
about which one is best. If you do development on small systems like your
laptop or stand alone system there is generally no Slurm and mpirun is the only
option, so mpirun will work on everything from Raspberry Pis through laptops to
Betzy.</p>
<p>Performance testing does not show any significant performance difference when
launching jobs in a normal way. There are, however a lot of possible
options to mpirun both OpenMPI and Intel MPI. Both environment flags and
command line options.</p>
</div>
</div>
</div>
<div class="section" id="library-settings">
<h2>Library settings<a class="headerlink" href="#library-settings" title="Permalink to this headline">¶</a></h2>
<p>The Intel MLK library performs run time checks at startup to select the
appropriate <em>INTEL</em> processor. When it cannot work ut the processor type a
least common instruction set is set is selected yielding lower performance.  To
instruct MKL to use a more suitable instruction set a debug variable can be
set, e.g. <code class="docutils literal notranslate"><span class="pre">export</span>&#160; <span class="pre">MKL_DEBUG_CPU_TYPE=5</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MKL_DEBUG_CPU_TYPE flag does not work for Intel compiler distribution<br />
2020 and and newer.</p>
</div>
<p>Users are adviced to check if there is any performance difference between
Intel 2019b and the 2020 versions. It’s not adviced to mix the Compiler and
MLK versions, e.g. building using 2020 and then link with MKL 2019.</p>
</div>
<div class="section" id="memory-architecture">
<h2>Memory architecture<a class="headerlink" href="#memory-architecture" title="Permalink to this headline">¶</a></h2>
<p>Betzy compute node is a 2-socket system running AMD EPYC 7742 64-Core
processors. Each node is a NUMA (<em>Non-Uniform Memory Access</em>) system, which
means that although all CPU cores can access all RAM, the speed of the access
will differ: some memory pages are closer to each CPU, and some are further
away. In contrast to a similar Intel-based system, where each socket is one
NUMA node, Betzy has 4 NUMA nodes per socket, and 8 in total. The NUMA
architecture can be obtained as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ numactl -H

available: 8 nodes (0-7)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
node 0 size: 32637 MB
node 0 free: 30739 MB
node 1 cpus: 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159
node 1 size: 32767 MB
node 1 free: 31834 MB
node 2 cpus: 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175
node 2 size: 32767 MB
node 2 free: 31507 MB
node 3 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
node 3 size: 32755 MB
node 3 free: 31489 MB
node 4 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
node 4 size: 32767 MB
node 4 free: 31746 MB
node 5 cpus: 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
node 5 size: 32767 MB
node 5 free: 31819 MB
node 6 cpus: 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239
node 6 size: 32767 MB
node 6 free: 31880 MB
node 7 cpus: 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255
node 7 size: 32767 MB
node 7 free: 31805 MB
node distances:
node   0   1   2   3   4   5   6   7
  0:  10  12  12  12  32  32  32  32
  1:  12  10  12  12  32  32  32  32
  2:  12  12  10  12  32  32  32  32
  3:  12  12  12  10  32  32  32  32
  4:  32  32  32  32  10  12  12  12
  5:  32  32  32  32  12  10  12  12
  6:  32  32  32  32  12  12  10  12
  7:  32  32  32  32  12  12  12  10
</pre></div>
</div>
<p>The above picture is further complicated by the fact that within the individual
NUMA nodes the memory access time is also not uniform. This can be verified by
running the STREAM benchmark. As reported above, each NUMA node has 16 physical
cores (e.g. node 0, cores 0-15). Consider the following 2 STREAM experiments:</p>
<ol class="simple">
<li><p>start 8 threads, bind them to cores 0-8</p></li>
<li><p>start 8 threads, bind them to cores 0,2,4,6,8,10,12,14</p></li>
</ol>
<p>In terms of the OMP_PLACES directive the above is equivalent to:</p>
<ol class="simple">
<li><p>OMP_PLACES=”{0:1}:8:1” OMP_NUM_THREADS=8 ./stream</p></li>
<li><p>OMP_PLACES=”{0:1}:8:2” OMP_NUM_THREADS=8 ./stream</p></li>
</ol>
<p>On a standard Intel-based system the above two experiments would perform
identical. This is not the case on Betzy: the first approach is slower than the
second one:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Experiment</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Best Rate MB/s</p></th>
<th class="head"><p>Avg time</p></th>
<th class="head"><p>Min time</p></th>
<th class="head"><p>Max time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Copy</p></td>
<td><p>37629.4</p></td>
<td><p>0.212833</p></td>
<td><p>0.212600</p></td>
<td><p>0.213007</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Triad</p></td>
<td><p>35499.6</p></td>
<td><p>0.338472</p></td>
<td><p>0.338032</p></td>
<td><p>0.338771</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Copy</p></td>
<td><p>42128.7</p></td>
<td><p>0.190025</p></td>
<td><p>0.189894</p></td>
<td><p>0.190152</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Triad</p></td>
<td><p>41844.4</p></td>
<td><p>0.287000</p></td>
<td><p>0.286777</p></td>
<td><p>0.287137</p></td>
</tr>
</tbody>
</table>
<p>This shows that the memory access time is not uniform within a single NUMA node.</p>
<p>Interestingly, the peak achievable memory bandwidth also depends on the number
of cores used, and is maximized for lower core counts. This is confirmed by the
following STREAM experiments running on one NUMA node</p>
<ol class="simple">
<li><p>start 8 threads, bind them to cores 0,2,4,6,8,10,12,14</p></li>
<li><p>start 16 threads, bind them to cores 0-15</p></li>
</ol>
<p>In terms of the OMP_PLACES directive the above is equivalent to</p>
<ol class="simple">
<li><p>OMP_PLACES=”{0:1}:8:2” OMP_NUM_THREADS=8 ./stream</p></li>
<li><p>OMP_PLACES=”{0:1}:16:1” OMP_NUM_THREADS=16 ./stream</p></li>
</ol>
<p>The results are:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Experiment</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Best Rate MB/s</p></th>
<th class="head"><p>Avg time</p></th>
<th class="head"><p>Min time</p></th>
<th class="head"><p>Max time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Copy</p></td>
<td><p>42126.3</p></td>
<td><p>0.190034</p></td>
<td><p>0.189905</p></td>
<td><p>0.190177</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Triad</p></td>
<td><p>41860.1</p></td>
<td><p>0.287013</p></td>
<td><p>0.286669</p></td>
<td><p>0.287387</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Copy</p></td>
<td><p>39675.8</p></td>
<td><p>0.201817</p></td>
<td><p>0.201634</p></td>
<td><p>0.201950</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Triad</p></td>
<td><p>39181.7</p></td>
<td><p>0.306733</p></td>
<td><p>0.306265</p></td>
<td><p>0.307508</p></td>
</tr>
</tbody>
</table>
<p>which shows that memory bandwidth is maximized when using 8 out of 16 cores per NUMA node.</p>
<p>The following experiments test the entire system:</p>
<ol class="simple">
<li><p>start 64 threads, bind them to cores 0,2,…126</p></li>
<li><p>start 128 threads, bind them to cores 0,1,..127</p></li>
</ol>
<p>In terms of the OMP_PLACES directive the above is equivalent to:</p>
<ol class="simple">
<li><p>OMP_PLACES=”{0:1}:64:2” OMP_NUM_THREADS=64 ./stream</p></li>
<li><p>OMP_PLACES=”{0:1}:128:1” OMP_NUM_THREADS=128 ./stream</p></li>
</ol>
<p>The results are:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Experiment</p></th>
<th class="head"><p>Function</p></th>
<th class="head"><p>Best Rate MB/s</p></th>
<th class="head"><p>Avg time</p></th>
<th class="head"><p>Min time</p></th>
<th class="head"><p>Max time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>Copy</p></td>
<td><p>334265.8</p></td>
<td><p>0.047946</p></td>
<td><p>0.047866</p></td>
<td><p>0.048052</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Triad</p></td>
<td><p>329046.7</p></td>
<td><p>0.073018</p></td>
<td><p>0.072938</p></td>
<td><p>0.073143</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Copy</p></td>
<td><p>315216.0</p></td>
<td><p>0.050855</p></td>
<td><p>0.050759</p></td>
<td><p>0.050926</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Triad</p></td>
<td><p>309893.4</p></td>
<td><p>0.077549</p></td>
<td><p>0.077446</p></td>
<td><p>0.077789</p></td>
</tr>
</tbody>
</table>
<p>Hence on Betzy memory bandwidth hungry applications will likely benefit from only using half of the cores (64).</p>
<p>Note however that it is not enough to just use 64 cores instead of 128. It is
crucial to bind the threads/ranks to cores correctly, i.e., to run on every
second core. So a correct binding is either 0,2,4,…,126, or 1,3,5,…,127.
The above assures that the application runs on both NUMA-nodes in the most
efficient way. If instead you run the application on cores 0-63 only, then it
will be running at 50% performance as only one NUMA node will be used.</p>
</div>
<div class="section" id="running-mpi-applications">
<h2>Running MPI applications<a class="headerlink" href="#running-mpi-applications" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Introduction<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>The MPI to use is selected at application build time. Two MPI implementations
are supported:</p>
<ul class="simple">
<li><p>Intel MPI</p></li>
<li><p>OpenMPI</p></li>
</ul>
<p>Behavior of Intel MPI can be adjusted through environment variables environment
variables, which start with <em>I_MPI</em>
(https://software.intel.com/content/www/us/en/develop/documentation/mpi-developer-reference-linux/top/environment-variable-reference.html)</p>
<p>OpenMPI uses both environment variables (which must be used when running
through <em>srun</em>) and command line options (for use with <em>mpirun</em>). Command line
options override both the config files and environment variables. For a
complete list of parameters run <code class="docutils literal notranslate"><span class="pre">ompi_info</span> <span class="pre">--param</span> <span class="pre">all</span> <span class="pre">all</span> <span class="pre">--level</span> <span class="pre">9</span></code>, or see
the documentation (https://www.open-mpi.org/faq/?category=tuning).</p>
</div>
<div class="section" id="binding">
<h3>Binding<a class="headerlink" href="#binding" title="Permalink to this headline">¶</a></h3>
<p>Since not all memory is equal some sort of binding to keep the process located on cores close to the memory is normally beneficial.</p>
<div class="section" id="intel-mpi">
<h4>Intel MPI<a class="headerlink" href="#intel-mpi" title="Permalink to this headline">¶</a></h4>
<p>Enable binding can be requested with an  environment flag, I_MPI_PIN=1 .</p>
<p>To limit the ranks to only the fist thread on SMT e.g. using only cores 0 to 127 set the Intel MPI environment variable I_MPI_PIN_PROCESSOR_EXCLUDE_LIST to 128-255, e.g.
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=128-255</span></code>.</p>
</div>
<div class="section" id="openmpi">
<h4>OpenMPI<a class="headerlink" href="#openmpi" title="Permalink to this headline">¶</a></h4>
<p>The simplest solution is just to request binding at the command line:
<code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">--bind-to</span> <span class="pre">core</span> <span class="pre">./a.out</span></code>
In this case binding to core is requested.  To learn more about the binding options try issuing the following command : <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">--help</span> <span class="pre">binding</span></code> .</p>
</div>
</div>
<div class="section" id="collective-mpi-operations-optimisation">
<h3>Collective MPI operations optimisation<a class="headerlink" href="#collective-mpi-operations-optimisation" title="Permalink to this headline">¶</a></h3>
<p>For OpenMPI the flag <em>OMPI_MCA_coll_hcoll_enable</em> to 0 to disable or 1 to
enable can have significant effect on the performance of your MPI application.
Most of the times it’s beneficial to enable it by including <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMPI_MCA_coll_hcoll_enable=1</span></code> in the run script.</p>
</div>
<div class="section" id="running-mpi-only-applications">
<h3>Running MPI only applications<a class="headerlink" href="#running-mpi-only-applications" title="Permalink to this headline">¶</a></h3>
<p>For well behaved MPI applications the job scripts are relatively simple. The
only important thing to notice is that processes (MPI ranks)  should be mapped
with one rank per two SMT threads (also often referred to a physical cores).</p>
<p>For both Intel and Open -MPI the simplest command line would be <code class="docutils literal notranslate"><span class="pre">mpirun</span> <span class="pre">./a.out</span></code> This will launch the MPI application with the default settings, while
not with optimal performance it will run the application using the resources
requested in the run script.</p>
<div class="section" id="id3">
<h4>Intel MPI<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>The variable I_MPI_PIN_PROCESSOR_EXCLUDE_LIST as mentioned earlier is good to
keep the ranks only on one of the two threads per processor core.  Intel uses
an environment variable to achieve the binding : I_MPI_PIN=1</p>
</div>
<div class="section" id="id4">
<h4>OpenMPI<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>For more optimal performance process binding can be introduced, like for
OpenMPI: <code class="docutils literal notranslate"><span class="pre">--bind-to</span> <span class="pre">core</span></code></p>
</div>
<div class="section" id="memory-bandwidth-sensitive-applications">
<h4>Memory bandwidth sensitive applications<a class="headerlink" href="#memory-bandwidth-sensitive-applications" title="Permalink to this headline">¶</a></h4>
<p>Some applications are very sensitive to memory bandwidth
and consequently will benefit from having fewer ranks per node than the number
of cores, like running only 64 ranks per node.
Using <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--ntasks-per-node=64</span></code> and then lauch using something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">--</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">slot</span><span class="p">:</span><span class="n">PE</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">core</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
<span class="n">mpirun</span> <span class="o">--</span><span class="nb">map</span><span class="o">-</span><span class="n">by</span> <span class="n">ppr</span><span class="p">:</span><span class="mi">32</span><span class="p">:</span><span class="n">socket</span><span class="p">:</span><span class="n">pe</span><span class="o">=</span><span class="mi">2</span> <span class="o">--</span><span class="n">bind</span><span class="o">-</span><span class="n">to</span> <span class="n">core</span> <span class="o">./</span><span class="n">a</span><span class="o">.</span><span class="n">out</span>
</pre></div>
</div>
<p>Tests have shown that more than 2x in performance is possible, but using twice
as many nodes. Twice the number of nodes will yield twice the aggregated
memory bandwith. Needless to say also twice as many core hours.</p>
</div>
</div>
<div class="section" id="running-mpi-openmp-hybrid-applications">
<h3>Running  MPI-OpenMP - Hybrid applications<a class="headerlink" href="#running-mpi-openmp-hybrid-applications" title="Permalink to this headline">¶</a></h3>
<p>For large core count a pure MPI solution is often not optimal. Like HPL (the
top500 test) the hybrid model is the highest performing case.</p>
<p>Most OpenMP or threaded programs respond to the environment variable
OMP_NUM_THREADS.  You want to set it or another variable like NPUS etc  set it
from a Slurm variable : <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK</span></code> or
<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">NCPUS=$SLURM_CPUS_PER_TASK</span></code>.</p>
<p>The mapping of ranks and OpenMP threads onto the cores on the compute node can
often be tricky. There are many ways of dealing with this, from the simplest
solution by just relying on the defaults to explicit placement of the ranks and
threads on precisely specified cores.</p>
<div class="section" id="id5">
<h4>Intel MPI<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<p>There are a lot of environment variables to be used with Intel MPI, they all start with I_MPI</p>
<ul class="simple">
<li><p>I_MPI_PIN</p></li>
<li><p>I_MPI_PIN_DOMAIN</p></li>
<li><p>I_MPI_PIN_PROCESSOR_EXCLUDE_LIST
The variable I_MPI_PIN_DOMAIN is good when running hybrid codes, setting it to
the number of threads per rank will help the launcher to place the ranks
correct. Setting I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=128-255 will make sure only
cores 0-127 are used for MPI ranks. This ensures that no two ranks
share the same physical core.</p></li>
</ul>
</div>
<div class="section" id="id6">
<h4>OpenMPI<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>There is currently some issues with mapping of threads started by MPI
processes. These threads are scheduled/placed on the same core as the MPI rank
itself. The issue seems to be an openMP issue with GNU OpenMP. We are working to
resolve this issue.</p>
</div>
</div>
<div class="section" id="create-a-hostfile-or-machinefile">
<h3>Create a hostfile or machinefile<a class="headerlink" href="#create-a-hostfile-or-machinefile" title="Permalink to this headline">¶</a></h3>
<p>To make a host or machinefile a simple  srun command can be used:
<code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">/bin/hostname</span> <span class="pre">|</span> <span class="pre">uniq</span></code></p>
<p>A more complicated example is a nodelist file for the molecular mechanics application NAMD :
<code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">/bin/hostname</span> <span class="pre">|</span> <span class="pre">uniq</span> <span class="pre">|</span> <span class="pre">awk</span> <span class="pre">-F\.</span> <span class="pre">'BEGIN</span> <span class="pre">{print</span> <span class="pre">“group</span> <span class="pre">main”};{print</span> <span class="pre">&quot;host</span> <span class="pre">&quot;,</span> <span class="pre">$1}'</span> <span class="pre">&gt;</span> <span class="pre">nodelist</span></code></p>
</div>
<div class="section" id="transport-options-openmpi">
<h3>Transport options OpenMPI<a class="headerlink" href="#transport-options-openmpi" title="Permalink to this headline">¶</a></h3>
<p>Most of the following is hidden behind some command line options, but in case
more information is needed about the subject of transport a few links will
provide more insight.</p>
<p>For detailed list of settings a good starting point is here :
https://www.open-mpi.org/faq/</p>
<p>OpenMPI 4.x uses UCX for transport, this is a communication library:
https://github.com/openucx/ucx/wiki/FAQ</p>
<p>Transport layer PML (point-to-point message layer) :
https://rivis.github.io/doc/openmpi/v2.1/pml-ob1-protocols.en.xhtml</p>
<p>Transport layer UCX :
https://www.openucx.org/ and https://github.com/openucx/ucx/wiki/UCX-environment-parameters</p>
<p>Collective optimisation library hcoll from Mellanox is also an option:
https://docs.mellanox.com/pages/viewpage.action?pageId=12006295</p>
<p>Setting the different devices and transports can be done using environment variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">UCX_IB_DEVICE_SPECS</span><span class="o">=</span><span class="mh">0x119f</span><span class="p">:</span><span class="mi">4115</span><span class="p">:</span><span class="n">ConnectX4</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span><span class="mh">0x119f</span><span class="p">:</span><span class="mi">4123</span><span class="p">:</span><span class="n">ConnectX6</span><span class="p">:</span><span class="mi">5</span>
<span class="n">export</span> <span class="n">UCX_NET_DEVICES</span><span class="o">=</span><span class="n">mlx5_0</span><span class="p">:</span><span class="mi">1</span>
<span class="n">export</span> <span class="n">OMPI_MCA_coll_hcoll_enable</span><span class="o">=</span><span class="mi">1</span>
<span class="n">export</span> <span class="n">UCX_TLS</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span><span class="n">sm</span><span class="p">,</span><span class="n">dc</span>
</pre></div>
</div>
<p>From the UCX documentation the list of internode transports include :</p>
<ul class="simple">
<li><p>rc</p></li>
<li><p>ud</p></li>
<li><p>dc</p></li>
</ul>
<p>The last one is Mellanox scalable offloaded dynamic connection transport.  The
self is a loopback transport to communicate within the same process, while sm
is all shared memory transports. There are two shared memory transports
installed</p>
<ul class="simple">
<li><p>cma</p></li>
<li><p>knem</p></li>
</ul>
<p>Selecting cma or knem might improve performance for applications that uses a
high number of MPI ranks per node. With 128 cores and possibly 128 MPI ranks
per node the intra node communication is quite important.</p>
<p>Depending on your application’s communication pattern, point-to-point or
collectives the usage of Mellanox optimised offload collectives can have an
impact.</p>
</div>
<div class="section" id="monitoring-process-thread-placement">
<h3>Monitoring process/thread placement<a class="headerlink" href="#monitoring-process-thread-placement" title="Permalink to this headline">¶</a></h3>
<p>To monitor the placement or ranks the <code class="docutils literal notranslate"><span class="pre">htop</span></code> utility is useful, just log in to
a node running your application and issue the <code class="docutils literal notranslate"><span class="pre">htop</span></code> command. By default  <code class="docutils literal notranslate"><span class="pre">htop</span></code>
numbers the cores from 1 through 256. This can be confusing at times (it can be
changed in htop by pressing F2 and navigate to display options and tick off
count from zero).</p>
<p>The 0-127 (assume you’ve ticked of the start to count from zero) are the first
one of the two SMT on the  AMD processor. The number of cores from 128 to 255
are the second SMT thread and share the same executional units as do the first
128 cores.</p>
<p>Using this view it’s easy to monitor how the ranks and threads are allocated
and mapped on the compressor cores.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="monitoring.html" class="btn btn-neutral float-right" title="Monitoring jobs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="job_scripts.html" class="btn btn-neutral float-left" title="Job Scripts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Sigma2/Metacenter

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>